{
  "articles": [
    {
      "path": "blog.html",
      "title": "Blog",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-02-15T01:44:18-08:00"
    },
    {
      "path": "index.html",
      "title": "Katelyn Toigo",
      "author": [],
      "contents": "\n\n          \n          \n          Katelyn Toigo\n          \n          \n          Home\n          Resume\n          Projects\n          Blog\n          ☰\n          \n          \n      \n        \n          \n            \n              \n            \n              Katelyn Toigo\n            \n            \n              \n                \n                    \n                      \n                        LinkedIn\n                      \n                    \n                  \n                                    \n                    \n                      \n                        GitHub\n                      \n                    \n                  \n                                    \n                    \n                      \n                        Email\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            \n            Summary\n            My determination to protect and restore biodiversity began as I grew up on an Illinois State Land and Water Reserve and traveled to national parks, refuges, and wilderness areas. While visiting Glacier National Park in 2008, I witnessed Ken Burns and his crew filming his national park documentary. When I viewed the film the next year, I was determined to make it to every U.S. national park. In June of 2016, a small Cessna landed on the sand dunes of Kobuk Valley National Park and my goal was achieved.\n            Earning a B.S. in Zoology and a B.S. in Geography and Environmental Resources from Southern Illinois University Carbondale laid the foundation to approach problems from both the perspective of societal needs and from the perspective of wildlife’s needs. Internships with The Nature Conservancy, the National Forest Foundation, Cypress Creek National Wildlife Refuge, and the National Great Rivers Research and Education Center enhanced skills in public education/outreach, collaboration, meeting facilitation, stewardship, and field methods. I am currently pursuing a Master of Environmental Science and Management degree at the University of California Santa Barbara Bren School, specializing in Conservation Planning.\n            \n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Katelyn Toigo\n            \n            \n              \n                \n                                    \n                    \n                      LinkedIn\n                    \n                  \n                                    \n                    \n                      GitHub\n                    \n                  \n                                    \n                    \n                      Email\n                    \n                  \n                                  \n              \n            \n            \n              \n              Summary\n              My determination to protect and restore biodiversity began as I grew up on an Illinois State Land and Water Reserve and traveled to national parks, refuges, and wilderness areas. While visiting Glacier National Park in 2008, I witnessed Ken Burns and his crew filming his national park documentary. When I viewed the film the next year, I was determined to make it to every U.S. national park. In June of 2016, a small Cessna landed on the sand dunes of Kobuk Valley National Park and my goal was achieved.\n              Earning a B.S. in Zoology and a B.S. in Geography and Environmental Resources from Southern Illinois University Carbondale laid the foundation to approach problems from both the perspective of societal needs and from the perspective of wildlife’s needs. Internships with The Nature Conservancy, the National Forest Foundation, Cypress Creek National Wildlife Refuge, and the National Great Rivers Research and Education Center enhanced skills in public education/outreach, collaboration, meeting facilitation, stewardship, and field methods. I am currently pursuing a Master of Environmental Science and Management degree at the University of California Santa Barbara Bren School, specializing in Conservation Planning.\n              \n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2022-02-15T01:44:18-08:00"
    },
    {
      "path": "projects.html",
      "title": "Projects",
      "description": "Projects Completed",
      "author": [],
      "contents": "\n1. Principal Components Analysis to Analyze Food Nutrients\nOverview\nThis script uses principal components analysis to analyze nutrient information for raw fruits and vegetables from the USDA National Nutrient Database (FoodDataCentral). Principal components analysis is an ordination method allowing us to understand as much about our multivariate data as possible in a simplified number of dimensions. Here, I’ll use the nutrients data from the USDA to explore variable relationships and clustering. I will only use the measurements that are in grams (protein, fat, carbohydrates, sugar, fiber), and I will only look at restaurant foods, baked products, and beef products.\nData citation: United States Department of Agriculture. https://fdc.nal.usda.gov/index.html\nAttach required packages:\n\n\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n \nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify) # For PCA biplot\nlibrary(patchwork)\nlibrary(broom)\n\n\n\nRead in the data\n\n\nnutrients <- read_csv(here(\"data\", \"usda_nutrients.csv\"))\n\n\n\nFilter the data\n\n\n# Filter food groups for restaurant foods, baked products, and beef products.\nnutrients_filter <- nutrients %>% \n  filter(FoodGroup == c(\"Restaurant Foods\", \"Baked Products\", \"Beef Products\"))\n\n\n\nRun the PCA\n\n\nnutrients_pca <- nutrients_filter %>% \n  select(ends_with('_g')) %>% \n  scale() %>% # scale the selected variables and pass to the prcomp() function\n  prcomp()\n\n\n\nCreate a biplot using autoplot\n\n\nautoplot(nutrients_pca,\n       data = nutrients_filter,\n       loadings = TRUE,\n       colour = 'FoodGroup', # colour needs to be British spelling\n       loadings.label = TRUE,\n       loadings.colour = \"black\",\n       loadings.label.colour = \"black\",\n       loadings.label.vjust = -0.5) +\n  theme_classic()\n\n\n\n\nFigure 1. PCA biplot between PC1 and PC2 when looking at protein, fiber, carbohydrates, sugar, and fat of baked products, beef products, and restaurant foods.\nSummary\nAll three food groups seem to have distinct clusters.\nBeef products associate more strongly with protein than with the other nutrients.\nFiber, carbs, and sugars tend to be more similar to each other than to protein. They are positively correlated.\nProtein is about 180 degrees different than carbs and sugars. This means they have a correlation of -1.\nFiber and fat have a correlation of 0 since they are about 90 degrees.\nBaked products are more similar to restaurant food than beef products based on these variables.\nOf the three clusters, beef products and baked products are the most different.\n2. Analysis of Seawater\nOverview\nThis analysis will look at the relationship between oxygen saturation of seawater off California’s coast and several physical and chemical variables including oxygen saturation, water temperature, water salinity, water depth, phosphate concentration, and nitrite concentrations.\nData citation: CalCOFI data are available for use without restriction. Data downloaded from https://calcofi.org/ccdata.html. Accessed 1/10/2022.\nAttach required packages\n\n\n# load packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(AICcmodavg)\nlibrary(equatiomatic)\n\n\n\nRead in the data\n\n\nseawater_samples <- read_csv(here('data', 'calcofi_seawater_samples.csv'))\n\n\n\nClean the data\n\n\nseawater_clean <- seawater_samples %>% \n  drop_na() %>% \n  rename(temp = t_deg_c, # variables are renamed so they are easier to work with and understand\n         depth = depth_m,\n         phosphate = po4u_m,\n         nitrite = no2u_m)\n\n\n\nCreate linear regression models using functions to store the equations\nModel 1 is oxygen saturation as a function of water temperature, salinity, and phosphate concentration.\nModel 2 is oxygen saturation as a function of water temperature, salinity, phosphate concentration, and depth.\nA third model has also been added. Model 3 is oxygen saturation as a function of water temperature, salinity, phosphate concentration, nitrite concentration, and depth.\n\n\nf1 <- o2sat ~ temp + salinity + phosphate\nmdl1 <- lm(f1, data = seawater_clean)\n\nf2 <- o2sat ~ temp + salinity + phosphate + depth\nmdl2 <- lm(f2, data = seawater_clean)\n\nf3 <- o2sat ~ temp + salinity + phosphate + + nitrite + depth\nmdl3 <- lm(f3, data = seawater_clean)\n\n\n\nUse AIC to compare the models\nAIC = Akaike Information Criterion\nModel with the lowest AIC is the preferred model. Ideally, the best model is lower than the next best model by at least 2.0. A difference of 2 indicates a significant difference in model fit.\n\n\nAICc(mdl1) # AICc() corrects for sample size\n\n\n[1] 619.0251\n\nAICc(mdl2)\n\n\n[1] 616.6048\n\nAICc(mdl3)\n\n\n[1] 613.5962\n\nAICcmodavg::aictab(list(mdl1, mdl2, mdl3))\n\n\n\nModel selection based on AICc:\n\n     K   AICc Delta_AICc AICcWt Cum.Wt      LL\nMod3 7 613.60       0.00   0.78   0.78 -299.19\nMod2 6 616.60       3.01   0.17   0.95 -301.85\nMod1 5 619.03       5.43   0.05   1.00 -304.19\n\nAIC of model 1 = 619.0251\nAIC of model 2 = 616.6048\nAIC of model 3 = 613.5962\nThe AIC of model 1 > model 2 > model 3.\nModel 3 is lower than model 2 by 3.01 and lower than model 1 by 5.43, so since it is lower by at least 2, model 3 is the preferred model.\nModel 3 is the preferred model via the AIC method.\nUse 10-fold cross validation to compare the models\nRoot-mean-square error is the scoring method.\n\n\nfolds <- 10 # number of folds\nfold_vec <- rep(1:folds, length.out = nrow(seawater_clean)) # fold vector is repeating over each fold\ntable(fold_vec)\n\n\nfold_vec\n 1  2  3  4  5  6  7  8  9 10 \n10 10 10 10 10 10 10 10 10 10 \n\nset.seed(42) # allows others to get same set of random numbers if they try to replicate this work\n\nseawater_fold <- seawater_clean %>% \n  mutate(group = sample(fold_vec, size = n(), replace = FALSE))\n\n\n\nCreate root-mean-square error function.\n\n\ncalc_rmse <- function(x, y) {\n  rmse_result <- (x-y)^2 %>% mean() %>% sqrt()\n  return(rmse_result)\n}\n\n\n\nCalculate over all folds and take the average.\n\n\nrmse_df <- data.frame() # Create a blank data frame.\n\n# Below is a for loop - loops through all 10 folds.\nfor(i in 1:folds) {\n  kfold_test_df <-seawater_fold %>% \n    filter(group == i) # in group i\n  kfold_train_df <-seawater_fold %>% \n    filter(group != i) # not in group i\n  \n  kfold_mdl1 <- lm(f1, data = kfold_train_df)\n  kfold_mdl2 <- lm(f2, data = kfold_train_df)\n  kfold_mdl3 <- lm(f3, data = kfold_train_df)\n  \n  kfold_pred_df <- kfold_test_df %>% \n    mutate(mdl1 = predict(kfold_mdl1, kfold_test_df),\n           mdl2 = predict(kfold_mdl2, .), # the period is a shortcut that says you are predicting on the data frame\n           mdl3 = predict(kfold_mdl3, .)) \n  kfold_rmse <- kfold_pred_df %>% \n    summarize(rmse_mdl1 = calc_rmse(mdl1, o2sat),\n              rmse_mdl2 = calc_rmse(mdl2, o2sat),\n              rmse_mdl3 = calc_rmse(mdl3, o2sat))\n  \n  #Store the last chunk above by combining with the blank data frame from above.\n  rmse_df <- bind_rows(rmse_df, kfold_rmse)\n}\n\nrmse_df %>% \n  summarize(mean_rmse_mdl1 = mean(rmse_mdl1),\n            mean_rmse_mdl2 = mean(rmse_mdl2),\n            mean_rmse_mdl3 = mean(rmse_mdl3))\n\n\n  mean_rmse_mdl1 mean_rmse_mdl2 mean_rmse_mdl3\n1       4.976605       4.876322       4.795063\n\nModel 1 RMSE = 4.976605\nModel 2 RMSE = 4.876322\nModel 3 RMSE = 4.795063\nModel 3 has the lowest root-mean-square error meaning it has the lowest error when predicting data.\nModel 3 is the preferred model via the k-fold cross validation method using RMSE scoring.\nBoth AIC and k-fold cross validation using root-mean-square error as the scoring method indicate model 3 to be the preferred model.\nThe coefficients for the final predictive model are identified:\n\n\nfinal_mdl <- lm(f3, data = seawater_clean)\nsummary(final_mdl)\n\n\n\nCall:\nlm(formula = f3, data = seawater_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.3273  -2.3744   0.0158   2.3748  19.4893 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 149.93538   93.39120   1.605   0.1117    \ntemp         -0.46764    0.41548  -1.126   0.2632    \nsalinity     -0.78711    2.92993  -0.269   0.7888    \nphosphate   -37.37466    2.45287 -15.237   <2e-16 ***\nnitrite      14.66437    6.46880   2.267   0.0257 *  \ndepth        -0.01789    0.01591  -1.124   0.2637    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.972 on 94 degrees of freedom\nMultiple R-squared:  0.9597,    Adjusted R-squared:  0.9575 \nF-statistic: 447.1 on 5 and 94 DF,  p-value: < 2.2e-16\n\nThe final model:\n\\[\n\\begin{aligned}\n\\operatorname{o2sat} &= \\alpha + \\beta_{1}(\\operatorname{temp}) + \\beta_{2}(\\operatorname{salinity}) + \\beta_{3}(\\operatorname{phosphate})\\ + \\\\\n&\\quad \\beta_{4}(\\operatorname{nitrite}) + \\beta_{5}(\\operatorname{depth}) + \\epsilon\n\\end{aligned}\n\\]\nWhere:\no2sat = oxygen saturation\ntemp = water temperature in degrees Celsius\nsalinity = salinity of the water\nphosphate = phosphate concentration in micro moles per liter\nnitrite = nitrite concentration in micro moles per liter\ndepth = depth in meters\nThe final model with numbers:\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{o2sat}} &= 149.94 - 0.47(\\operatorname{temp}) - 0.79(\\operatorname{salinity}) - 37.37(\\operatorname{phosphate})\\ + \\\\\n&\\quad 14.66(\\operatorname{nitrite}) - 0.02(\\operatorname{depth})\n\\end{aligned}\n\\]\n3. Binary Logistic Regression to Identify Palmetto Species\nOverview\nThis script uses variables such as plant height, canopy length, canopy width, and the number of green leaves to classify whether palmettos are of species Serenoa repens or Sabal etonia. Binary logistic regression is used to test feasibility.\nData was collected from 1981 to 2017 in south-central Florida. It includes information regarding year, plant type, species, site, habitat, treatment, survival, height, canopy length, canopy width, number of green leaves, scape, new leaves, canopy, if long, comments, and biomass.\nData citation: Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(GGally)\nlibrary(broom)\nlibrary(jtools)\nlibrary(caret)\nlibrary(AICcmodavg)\nlibrary(data.table)\nlibrary(knitr)\n\n\n\n\n\npalmetto <- read_csv(here(\"data\", \"palmetto.csv\")) # Read in the data\n\n\n\nWrangle the data\nSelect columns for use in the analysis, change values in the species column to species names, and convert the species column to a factor class.\n\n\npalmetto_clean <- palmetto %>% # Clean the data table\n  select(species, height:green_lvs) %>% # Select only the columns I need for analysis, the colon means \"all the way over to\"\n  drop_na()\n\n# Change the values 1 and 2 in the species column to actual species names. 1 = Serenoa repens, 2 = Sabal etonia\npalmetto_clean$species <- ifelse(palmetto_clean$species==1,'Serenoa repens','Sabal etonia') \n\n# Convert the  species column to a factor class\npalmetto_clean$species <- as.factor(palmetto_clean$species) \n\n# levels(palmetto_clean$species) in the console revealed Sabal etonia is factor level 0 and Serenoa repens is factor level 1\n\n\n\nVisualize the data to identify trends.\nDifferences in height, canopy length, canopy width, and green leaves for Serenoa repens or Sabal etonia.\n\n\nggplot(data = palmetto_clean, aes(x = height, y = length)) +\n  geom_point(aes(color = species)) + \n  labs(title = 'Distribution of Height vs. Canopy Length of Two Palmetto Species', \n       x = 'Plant Height (cm)', \n       y = 'Canopy Length (cm)', \n       color = \"Species\") + # title, axis labels, and legend title\n  theme_minimal() # sets the theme of the graph - for visual purposes\n\n\n\n\nFigure 1. The relationship between canopy length (cm) and maximum plant height (cm) is shown for each species. Sabal etonia is shown in red. Serenoa repens is shown in blue.\n\n\nggplot(data = palmetto_clean, aes(x = width, y = green_lvs)) +\n  geom_point(aes(color = species)) + \n  labs(title = 'Distribution of Canopy Width vs. Number of Green Leaves of Two Palmetto Species', \nx = 'Canopy Width (cm)', \ny = 'Number of Green Leaves', \ncolor = \"Species\") + # title, axis labels, and legend title\n  theme_minimal() # sets the theme of the graph - for visual purposes\n\n\n\n\nFigure 2. The relationship between the number of green leaves and canopy width (cm) is shown for each species. Sabal etonia is shown in red. Serenoa repens is shown in blue.\nBased on these plots, the number of green leaves is the most likely variable to help classify the species correctly. It shows the most difference between species. Canopy width, plant height, and canopy length showed similar trends between species.\nBinary logistic regression\n\n\n# Store formulas\nf1 <- species ~ height + length + width + green_lvs\n\nf2 <- species ~ height + width + green_lvs\n\n\n\n\n\n# Binomial logistic regression for model 1 which includes height + length + width + green_lvs.\npalm_blr1 <- glm(formula = f1, \n                    data = palmetto_clean,\n                    family = 'binomial')\n\n\n\n\n\n# Binomial logistic regression for model 2 which includes height + width + green_lvs and does not include length.\npalm_blr2 <- glm(formula = f2, \n                    data = palmetto_clean,\n                    family = 'binomial')\n\n\n\nIdentify which model performs better\nCompare the models using AICc\n\n\nAICcmodavg::aictab(list(palm_blr1, palm_blr2)) # this function compares competing models\n\n\n\nModel selection based on AICc:\n\n     K    AICc Delta_AICc AICcWt Cum.Wt       LL\nMod1 5 5194.57       0.00      1      1 -2592.28\nMod2 4 5987.48     792.91      0      1 -2989.74\n\nModel 1 is better because the AIC for model 1 (5194.57) is more than two less than the AIC of model 2 (5987.48).\nCompare with a 10-fold cross-validation, using prediction accuracy as our metric.\n\n\n# Using `caret` (\"**C**lassification **A**nd **RE**gression **T**raining\"):\nset.seed(123) \n\n# tr_ctrl <- trainControl(method = \"cv\", number = 10)\ntr_ctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 10)\n\n# Train the model\nmodel1 <- train(f1, data = palmetto_clean, \n               method = \"glm\", family = 'binomial',\n               trControl = tr_ctrl)\n\nmodel2 <- train(f2, data = palmetto_clean, \n               method = \"glm\", family = 'binomial',\n               trControl = tr_ctrl)\n\n\n\nModel 1 (92% accurate) is more accurate than model 2 (90% accurate) according to 10-fold cross validation.\nBased on both AIC values and 10-fold cross validation, model 1 is the better model.\nTrain model 1 using the entire dataset, and create a finalized table containing the binary logistic regression model results\n\n\nfinal_mdl <- glm(formula = f1, \n                 data = palmetto_clean, \n                 family = 'binomial')\n\n\n\nTable 1. Model 1 binary logistic regression model results.\n\n\n### Get a tidy version w/ broom:\nfinal_mdl_tidy <- broom::tidy(final_mdl)\n\nsetnames(final_mdl_tidy, old=c(\"term\",\"estimate\", \"std.error\", \"statistic\", \"p.value\"), new=c(\"Term\",\"Estimate\", \"Standard Error\", \"z-value\", \"p-value\"))\n\nkable(final_mdl_tidy)\n\n\nTerm\nEstimate\nStandard Error\nz-value\np-value\n(Intercept)\n-3.2266851\n0.1420708\n-22.71180\n0\nheight\n0.0292173\n0.0023061\n12.66984\n0\nlength\n-0.0458233\n0.0018661\n-24.55600\n0\nwidth\n-0.0394434\n0.0021000\n-18.78227\n0\ngreen_lvs\n1.9084747\n0.0388634\n49.10728\n0\n\nThe model is predicting the likelihood that the species is a Serenoa repens because the factor level of Serenoa repens is 1.\nAll variables are statistically significant.\nHigher length and width would be less likely to be Serenoa repens because the value is negative.\nHigher height and more green leaves would be more likely to be a Serenoa repens because the value is positive. This makes sense when looking at graphs above.\nEvaluation of how successfully this model would “classify” a plant as the correct species, using a 50% cutoff.\nTable 2. Evaluation of how successfully this model would “classify” a plant as the correct species, using a 50% cutoff.\n\n\nblr1_fitted <- palm_blr1 %>% # fitted column reports the probability that an individual is a Serenoa repens\n  broom::augment(type.predict = \"response\") %>% # instead of taking the log odds we are taking the odds and converting them into a probability\n  mutate(predicted_species = ifelse(.fitted >= 0.5, \"Serenoa repens\", \"Sabal etonia\")) %>% \n  group_by(species) %>% # separate by species\n  summarize(number_correctly_identified = sum(species == predicted_species),\n            number_total = n()) %>% # number_total is needed to identify percent correctly identified\n  mutate(percent_correctly_identified = number_correctly_identified/number_total * 100) %>% \n  select(species, number_correctly_identified, percent_correctly_identified)\n\nsetnames(blr1_fitted, old=c(\"species\",\"number_correctly_identified\", \"percent_correctly_identified\"), new=c(\"Species\",\"Number Correctly Identified\", \"Percent Correctly Identified\"))\nkable(blr1_fitted)\n\n\nSpecies\nNumber Correctly Identified\nPercent Correctly Identified\nSabal etonia\n5701\n92.62388\nSerenoa repens\n5548\n90.77225\n\nA binary logistic regression model using plant height, canopy length, canopy width and green leaves as predictor variables is better at predicting palmetto species (Serenoa repens and Sabal etonia) than a model using plant height, canopy width and the number of green leaves. The model correctly identified 5701 (92.62%) of Sabal etonia correctly and 5548 (90.77%) of Serenoa repens correctly.\n4. Non-Linear Least Squares to Analyze Lizard Populations\nOverview\nThis script uses non linear least squares to estimate parameters of a length to weight model for lizard populations in New Mexico.\nData was collected from 1989 to 2006 on 11 NPP study locations at the Jornada Basin LTER. The dataset includes information regarding species, sex, snout vent length, and weight.\nData citation: Lightfoot, D. and W.G. Whitford. 2020. Lizard pitfall trap data from 11 NPP study locations at the Jornada Basin LTER site, 1989-2006 ver 37. Environmental Data Initiative. https://doi.org/10.6073/pasta/4a6e258fb49c31e222ecbbcfd128967f\nLoad required packages\n\n\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(Metrics)\nlibrary(cowplot)\nlibrary(here)\nlibrary(data.table)\nlibrary(knitr)\n\n\n\nRead in the data\n\n\nlizard <- read_csv(here(\"data\", \"lizard.csv\")) # Read in the data\n\n\n\nCreate a model\nSnout length to weight model: W = a(SVL)^b\nW = weight\nSVL = snout to vent length\na & b = parameters that need to be fitted\n\n\ncalc_weight <- function(a, SV_length, b){\n W = a*(SV_length)^b\n \n return(W)\n}\n\n\n\nAn initial guess must be made\nThe model is exponential, so data will be log transformed and an OLS regression will be used to approximate parameters.\nData is log transformed\nSV_length and weight are log transformed\n\n\n# Add two new columns: log SVL and log weight.\nlizard_log <- lizard %>% \n  mutate(log_length = log(lizard$SV_length)) %>% \n  mutate(log_weight = log(lizard$weight))\n\n\n\nOLS regression\n\n\nguess_model <- lm(log_weight ~ log_length, data = lizard_log)\n\n\n\nAll species NLS - NLS model of all species starting with coefficients from OLS regression\n\n\nlizard_nls <- nls(weight ~ calc_weight(a, SV_length, b),\n                  data = lizard,\n                  start = list(a = exp(coefficients(guess_model)[1]), # starting values identified in OLS function above. a was found by taking the exponent of the intercept value.\n                               b = coefficients(guess_model)[2]), \n                  trace = TRUE)\n\n\n17064.55    (3.24e-01): par = (0.0002085246 2.537119)\n16502.11    (2.59e-01): par = (0.0002772028 2.477012)\n15516.47    (5.68e-02): par = (0.0003473554 2.443989)\n15466.89    (2.62e-03): par = (0.000337407 2.45569)\n15466.81    (6.77e-04): par = (0.000342001 2.452585)\n15466.80    (1.56e-04): par = (0.0003409034 2.453348)\n15466.80    (3.81e-05): par = (0.0003411808 2.453161)\n15466.80    (9.33e-06): par = (0.0003411133 2.453207)\n\n#tidy model output\nlizard_nls_tidy <- broom::tidy(lizard_nls)\n\nsetnames(lizard_nls_tidy, old=c(\"term\",\"estimate\", \"std.error\", \"statistic\", \"p.value\"), new=c(\"Term\",\"Estimate\", \"Standard Error\", \"z-value\", \"p-value\")) # changes names of columns on table\n\nkable(lizard_nls_tidy) # creates clean table\n\n\nTerm\nEstimate\nStandard Error\nz-value\np-value\na\n0.0003411\n0.0000400\n8.538421\n0\nb\n2.4532065\n0.0269791\n90.930011\n0\n\nPlot of fitted NLS model and lizards separated by sex.\n\n\npredict1 <- predict(lizard_nls)\n\nall_sp_complete <- data.frame(lizard, predict1) # adds prediction column to original data table\n  \nggplot(data = all_sp_complete, aes(x = SV_length, y = weight)) +\n  geom_point(aes(color = sex)) +\n  geom_line(aes(x = SV_length, y = predict1),\n            color = \"black\",\n            size = 1) +\n  labs(x = \"Snout to Vent Length (mm)\",\n       y = \"Weight (g)\",\n       title = \"Western Whiptail Males and Nonlinear Least Squares Model Predictions\") +\n  theme_minimal()\n\n\n\n\nFigure 1. NLS model overlaid with lizard data separated by sex.\nRMSE Function\n\n\n# define RMSE function\ncalc_rmse <- function(x, y) {\n  rmse_result <- (x-y)^2 %>% mean() %>% sqrt()\n  return(rmse_result)\n}\n\n\n\nRMSE of all species NLS\n\n\nrmse_predict_all_sp <- all_sp_complete %>% \n  summarize(rmse_all_sp = calc_rmse(weight, predict1))\n\nrmse_predict_all_sp\n\n\n  rmse_all_sp\n1    2.790684\n\nWestern whiptail lizard NLS\n\n\n# Filter male western whiptail lizards\nwestern_whiptails <- lizard_log %>% \n  filter(spp == \"CNTI\") %>% \n  filter(sex == \"M\")\n\n\n\nOLS regression to identify coefficients to use for western whiptail gueses\n\n\nww_guess_model <- lm(log_weight ~ log_length, data = western_whiptails)\n\n\n\nNLS model of western whiptails starting with coefficients from OLS regression\n\n\nww_nls <- nls(weight ~ calc_weight(a, SV_length, b), # compare weight to the calc_weight function\n                  data = western_whiptails,\n                  start = list(a = exp(coefficients(ww_guess_model)[1]), \n                               b = coefficients(ww_guess_model)[2]), \n                  trace = TRUE)\n\n\n849.1572    (2.38e-01): par = (0.0001204877 2.698108)\n845.6613    (2.27e-01): par = (0.0001330208 2.674996)\n843.4176    (2.19e-01): par = (0.000145976 2.653427)\n841.8629    (2.14e-01): par = (0.0001592699 2.633319)\n840.6339    (2.09e-01): par = (0.0001728179 2.614592)\n839.5079    (2.05e-01): par = (0.0001865362 2.597165)\n838.3596    (2.00e-01): par = (0.0002003434 2.580958)\n837.1280    (1.96e-01): par = (0.0002141623 2.565892)\n835.7939    (1.91e-01): par = (0.0002279206 2.551894)\n834.3628    (1.85e-01): par = (0.0002415522 2.538892)\n832.8544    (1.80e-01): par = (0.0002549974 2.526816)\n831.2942    (1.74e-01): par = (0.0002682034 2.515604)\n830.9229    (1.72e-01): par = (0.0002940453 2.494784)\n829.4806    (1.66e-01): par = (0.0003185494 2.476905)\n827.3386    (1.57e-01): par = (0.0003414678 2.461569)\n826.9172    (1.55e-01): par = (0.0003838388 2.435273)\n822.7939    (1.37e-01): par = (0.0004187718 2.416406)\n818.0645    (1.14e-01): par = (0.0004743039 2.389404)\n809.0350    (4.10e-02): par = (0.0005336902 2.365838)\n807.6757    (1.94e-04): par = (0.0005309859 2.368548)\n807.6756    (1.74e-05): par = (0.0005316264 2.368276)\n807.6756    (1.56e-06): par = (0.0005315662 2.368301)\n\n#tidy model output\nww_nls_tidy <- broom::tidy(ww_nls)\n\nsetnames(ww_nls_tidy, old=c(\"term\",\"estimate\", \"std.error\", \"statistic\", \"p.value\"), new=c(\"Term\",\"Estimate\", \"Standard Error\", \"z-value\", \"p-value\"))\n\nkable(ww_nls_tidy)\n\n\nTerm\nEstimate\nStandard Error\nz-value\np-value\na\n0.0005316\n0.0004288\n1.239686\n0.2192313\nb\n2.3683013\n0.1800348\n13.154689\n0.0000000\n\n\n\npredict <- predict(ww_nls)\n\nww_complete <- data.frame(western_whiptails, predict) # adds prediction column to original data table\n\n\n\nRMSE for western whiptail NLS\n\n\nrmse_predict_ww <- ww_complete %>% \n  summarize(rmse_ww = calc_rmse(weight, predict))\n\nrmse_predict_ww\n\n\n   rmse_ww\n1 3.349286\n\nPlot of the output from the western whiptail NLS model to the general nls model for all species by graphing the model fits on the Western Whiptail male data.\n\n\nggplot(data = ww_complete, aes(x = SV_length, y = weight)) +\n  xlim(40, 110) + # limit x axis SV_length values to min and max western whiptail SVL's for better visualization\n  ylim(0, 40) + # limit y axis weight values for better visualization\n  geom_point() + # display western whiptail male points\n  geom_line(aes(x = SV_length, y = predict), # add western whiptail NLS line\n            color = \"blue\",\n            size = 1) +\n  geom_line(data = all_sp_complete, aes(x = SV_length, y = predict1), # add all sp. NLS line\n            color = \"red\",\n            size = 1) +\n  labs(x = \"Snout to Vent Length (mm)\",\n       y = \"Weight (g)\",\n       title = \"Western Whiptail NLS Model and All Species NLS Model Displayed with Western Whiptail Male Data Points\") +\n  theme_minimal()\n\n\n\n\nFigure 2. NLS model for all species (red) and NLS model for western whiptail lizards (blue) plotted against male western whiptail datapoints. The RMSE for the all species NLS is 2.790684. The RMSE for western whiptail NLS is 3.349286. The model for all species should be used because it has a lower RMSE and fits the data better.\n\n\n\n",
      "last_modified": "2022-02-15T01:44:59-08:00"
    },
    {
      "path": "Resume.html",
      "title": "Resume",
      "description": "Interests, Education, Experience, and More",
      "author": [],
      "contents": "\nInterests\nI am keenly interested in working with private landowners to restore and conserve their land and, ultimately, envision myself working for a land trust.\nIn my free time, you will find me hiking, cooking, painting, visiting national parks, and spending as much time with my dog, Basil, as possible.\nEducation\nUniversity of California, Santa Barbara | Santa Barbara, CA\nMaster of Environmental Science and Management | September 2020 - June 2022\nSouthern Illinois University, Carbondale | Carbondale, IL\nB.S. in Zoology & B.S. in Geography and Environmental Resources | August 2016 - May 2020\nExperience\nConservation Connect Fellow | National Forest Foundation | May 2021 - May 2022\nDesigned a business plan for the Southern Idaho Forest Fund\nConducted meetings with U.S. Forest Service personnel\nCompiled internal and external reports about southern Idaho National Forests shared with the Washington D.C. U.S. Forest Service office\nTook notes during meetings\nCreated a direct mail postcard which was sent to residents of the Wood River Valley\nAssisted in a media tour and community engagement event\nWrote a blog post\nFloodplain Restoration GLOBE Intern | The Nature Conservancy | June 2019 - August 2019\nWorked with the U.S. Fish and Wildlife Service and Illinois Natural History Survey.\nConducted a public use survey and wrote a final summary report.\nCreated seamless stewardship data maps using ArcGIS and Collector.\nProjects\nPoster Presented at the World Congress of Malacology | August 2019\nEffects of Strip Mine Reclamation Practices on Terrestrial Snail Communities\n\n\n\n",
      "last_modified": "2022-02-15T01:44:59-08:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
